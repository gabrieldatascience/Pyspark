{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP y NLTK Basicos\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext y SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"Python Spark SQL basic example\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos de los ejemplos de este artículo han sido tomados del libro escrito por **Bird et al. (2009)**. Aquí traté de poner en práctica los ejemplos del libro con la mayor chispa posible.\n",
    "\n",
    "Consulte el libro para más detalles: Bird, Steven, Ewan Klein y Edward Loper. Procesamiento de lenguaje natural con Python: análisis de texto con el kit de herramientas de lenguaje natural. \"O'Reilly Media, Inc.\", 2009.\n",
    "\n",
    "## Terminología básica\n",
    "\n",
    "* **text**: una secuencia de palabras y puntuación.\n",
    "* **frequency distribution**: la frecuencia de las palabras en un objeto de texto.\n",
    "* **collocation**: a **sequence of words** que ocurren juntos inusualmente a menudo.\n",
    "* **bigrams**: pares de palabras. Los bigrams de alta frecuencia son colocaciones.\n",
    "* **corpus**: un gran cuerpo de texto\n",
    "* **wordnet**: una base de datos léxica en la que las palabras en inglés se agrupan en conjuntos de sinónimos (**también llamados synsets**).\n",
    "* **text normalization**: el proceso de transformar el texto en una sola forma canónica, por ejemplo, convertir el texto en minúsculas, eliminar las puntuaciones, etc.\n",
    "* **Lemmatization**: el proceso de agrupar formas de variantes de la misma palabra para que puedan analizarse como una única posición.\n",
    "* **Stemming**: el proceso de reducir las palabras inflexionadas a su **tronco de palabras**.\n",
    "* **tokenization**:\n",
    "* **segmentation**:\n",
    "* **chunking**:\n",
    "\n",
    "## Textos como listas de palabras\n",
    "\n",
    "Cree un marco de datos compuesto de elementos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|texts                                   |\n",
      "+----------------------------------------+\n",
      "|[I, like, playing, basketball]          |\n",
      "|[I, like, coding]                       |\n",
      "|[I, like, machine, learning, very, much]|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'texts': [['I', 'like', 'playing', 'basketball'],\n",
    "                 ['I', 'like', 'coding'],\n",
    "                 ['I', 'like', 'machine', 'learning', 'very', 'much']]\n",
    "    })\n",
    "    \n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngramas y coubicaciones\n",
    "\n",
    "Transformar textos en colocaciones de 2, 3 y 4 gramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml import Pipeline\n",
    "ngrams = [NGram(n=n, inputCol='texts', outputCol=str(n)+'-grams') for n in [2,3,4]]\n",
    "\n",
    "# construir modelo pipeline \n",
    "pipeline = Pipeline(stages=ngrams)\n",
    "\n",
    "# transformar datos\n",
    "texts_ngrams = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|2-grams                                                           |\n",
      "+------------------------------------------------------------------+\n",
      "|[I like, like playing, playing basketball]                        |\n",
      "|[I like, like coding]                                             |\n",
      "|[I like, like machine, machine learning, learning very, very much]|\n",
      "+------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|3-grams                                                                           |\n",
      "+----------------------------------------------------------------------------------+\n",
      "|[I like playing, like playing basketball]                                         |\n",
      "|[I like coding]                                                                   |\n",
      "|[I like machine, like machine learning, machine learning very, learning very much]|\n",
      "+----------------------------------------------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------+\n",
      "|4-grams                                                                          |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|[I like playing basketball]                                                      |\n",
      "|[]                                                                               |\n",
      "|[I like machine learning, like machine learning very, machine learning very much]|\n",
      "+---------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostrar resultado\n",
    "texts_ngrams.select('2-grams').show(truncate=False)\n",
    "texts_ngrams.select('3-grams').show(truncate=False)\n",
    "texts_ngrams.select('4-grams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceso a los corpus desde el paquete NLTK\n",
    "\n",
    "### El corpus `gutenberg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener identificadores de archivos en gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg_fileids = gutenberg.fileids()\n",
    "gutenberg_fileids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vía de acceso absoluta de un fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/mingchen/nltk_data/corpora/gutenberg/austen-emma.txt')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.abspath(gutenberg_fileids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; an'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.raw(gutenberg_fileids[0])[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Las palabras de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentencias de un expediente específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.sents(gutenberg_fileids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7752"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.sents(gutenberg_fileids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de corpus personalizado\n",
    "\n",
    "Vamos a crear un corpus que contenga todos los archivos del directorio **./data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_data = PlaintextCorpusReader('./data', '.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archivos en el corpus *corpus_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Advertising.csv',\n",
       " 'Credit.csv',\n",
       " 'WineData.csv',\n",
       " 'churn-bigml-20.csv',\n",
       " 'churn-bigml-80.csv',\n",
       " 'cuse_binary.csv',\n",
       " 'horseshoe_crab.csv',\n",
       " 'hsb2.csv',\n",
       " 'hsb2_modified.csv',\n",
       " 'iris.csv',\n",
       " 'mtcars.csv',\n",
       " 'prostate.csv',\n",
       " 'twitter.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fileids = corpus_data.fileids()\n",
    "data_fileids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw text en *twitter.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fresh install of XP on new computer. Sweet relief! fuck vista\\t1018769417\\t1.0\\nWell. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl\\t10284216536\\t1.0\\n\"Literally six weeks before I can take off \"\"SSC Chair\"\" off my email. Its like the torturous 4th mile before everything stops hurting.\"\\t10298589026\\t1.0\\nMitsubishi i MiEV - Wikipedia, the free encyclopedia - http://goo.gl/xipe Cutest car ever!\\t109017669432377344\\t1.0\\n\\'Cheap Eats in SLP\\' - http://t.co/4w8gRp7\\t109642968603963392\\t1.0\\nTeenage Mutant Ninja Turtle art is never a bad thing... http://bit.ly/aDMHyW\\t10995492579\\t1.0\\nNew demographic survey of online video viewers: http://bit.ly/cx8b7I via @KellyOlexa\\t11713360136\\t1.0\\nhi all - i\\'m going to be tweeting things lookstat at the @lookstat twitter account. please follow me there\\t1208319583\\t1.0\\nHoly carp, no. That movie will seriously suffer for it. RT @MouseInfo: Anyone excited for The Little Mermaid in 3D?\\t121330835726155776\\t1.0\\n\"Did I really need to learn \"\"I bought a box and put in it things\"\" in arabic? This is the most random book ever.\"\\t12358025545\\t1.0\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.raw('twitter.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Palabras y frases en archivo *twitter.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fresh', 'install', 'of', 'XP', 'on', 'new', ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.words(fileids='twitter.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_data.words(fileids='twitter.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fresh', 'install', 'of', 'XP', 'on', 'new', 'computer', '.'], ['Sweet', 'relief', '!'], ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.sents(fileids='twitter.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_data.sents(fileids='twitter.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "La función `nltk.corpus.wordnet.synsets()` carga todos los sintetizadores con un determinado lema y parte de la etiqueta de voz.\n",
    "\n",
    "Carga todos los synsets en un marco de datos de chispa dado el lema `car`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|   car_synsets|\n",
      "+--------------+\n",
      "|      car.n.01|\n",
      "|      car.n.02|\n",
      "|      car.n.03|\n",
      "|      car.n.04|\n",
      "|cable_car.n.01|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets\n",
    "pdf = pd.DataFrame({\n",
    "        'car_synsets': [synsets._name for synsets in wordnet.synsets('car')]\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener nombres de lemas a los que se les ha dado un synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'railcar', 'railway_car', 'railroad_car']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def lemma_names_from_synset(x):\n",
    "    synset = wordnet.synset(x)\n",
    "    return synset.lemma_names()\n",
    "\n",
    "lemma_names_from_synset('car.n.02')\n",
    "# synset_lemmas_udf = udf(lemma_names_from_synset, ArrayType(StringType()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
