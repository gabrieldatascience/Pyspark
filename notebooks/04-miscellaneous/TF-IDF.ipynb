{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear SparkContext y SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"Python Spark SQL basic example\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF, IDF y TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF es la abreviatura de **Frecuencia de Término**. Es simplemente la frecuencia de un término en un documento. Cuanto más alta sea la TF para un término específico, más importante será ese término para ese documento.\n",
    "\n",
    "* IDF es la abreviatura de **Inverse Document Frequency**. Es la frecuencia de los documentos que contienen un término específico. Si existe un término en cada documento, entonces la Frecuencia de documentos es la mayor y es 1. Y la Frecuencia inversa de documentos será la menor. En esta situación, este término no es informativo para clasificar los documentos. Cuanto más alto es el IDF, más relavante es el término.\n",
    "\n",
    "* TF-IDF es el producto de TF e IDF. Se obtiene un TF-IDF alto cuando el Término Frecuencia es alto y la Frecuencia de Documento es baja (IDF es alta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Término Frecuencia, HashingTF y CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark tiene dos funciones para calcular frecuencias de términos a partir de documentos: el **`HashingTF()`** y el **`CountVectorizer()`**. Estas dos funciones hacen dos cosas:\n",
    "\n",
    "1. Indexación de términos: conversión de palabras a números.\n",
    "2. Calcule las frecuencias de los plazos para cada documento.\n",
    "\n",
    "El `HashingTF()` utiliza la función Murmurhash 3 para mapear una característica sin procesar (un término) en un índice (un número). El hashing es el proceso de transformar datos de tamaño arbitrario en datos de tamaño fijo, generalmente más cortos. Las frecuencias de término se calculan en base a los índices generados. Para el método HashingTF(), el proceso de mapeo es muy barato. Porque cada mapeo de término a índice es independiente de otros mapeos de término a índice. La función de hashing toma una entrada única y genera un \"resultado único\". Sin embargo, puede ocurrir una colisión**, lo que significa que diferentes características (términos) pueden ser ajustados al mismo índice.\n",
    "\n",
    "El **`CountVectorizer()`** indexa los términos por orden descendente de frecuencias de términos en todo el corpus, NO las frecuencias de términos en el documento. Después del proceso de indexación, las frecuencias de los términos se calculan mediante documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|terms                                      |\n",
      "+-------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|\n",
      "|[I, love, spark, very, very, much]         |\n",
      "|[everyone, should, use, spark]             |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'terms': [\n",
    "            ['spark', 'spark', 'spark', 'is', 'awesome', 'awesome'],\n",
    "            ['I', 'love', 'spark', 'very', 'very', 'much'],\n",
    "            ['everyone', 'should', 'use', 'spark']\n",
    "        ]\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF\n",
    "\n",
    "El **numFeatures** toma un número entero, que debe ser mayor que el número total de términos en el corpus. Y debería ser una potencia de dos para que las características se asignen uniformemente a las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashtf = HashingTF(numFeatures=pow(2, 4), inputCol='terms', outputCol='features(numFeatures), [index], [term frequency]')\n",
    "stages = [hashtf]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------+\n",
      "|terms                                      |features(numFeatures), [index], [term frequency]|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(16,[1,15],[4.0,2.0])                           |\n",
      "|[I, love, spark, very, very, much]         |(16,[0,1,2,8,12],[1.0,1.0,1.0,2.0,1.0])         |\n",
      "|[everyone, should, use, spark]             |(16,[1,9,13],[2.0,1.0,1.0])                     |\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede notar que el primer documento tiene tres términos distintos, pero sólo se obtienen dos frecuencias de términos. Esta aparente discrepancia se debe a una colisión **hashing**: tanto `spark` como `is` se están apresurando a `1`. La frecuencia del término para el índice `1` en el primer documento es `4.0` correspondiente a los tres conteos de `spark` y el conteo de `is`. La probabilidad de una colisión de hashing puede reducirse aumentando el parámetro `numFeatures` pasado a la función `HashingTF` (el valor por defecto es por ejemplo $2^18 = 262,144$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "La función **`CountVectorizer()`** tiene tres parámetros para controlar qué términos se mantendrán como características.\n",
    "\n",
    "* minTF: se eliminarán las características que tengan una frecuencia de término inferior a minTF. Si minTF=1minTF=1, no se eliminará ninguna característica.\n",
    "* minDF: se eliminarán las características que tengan una frecuencia de documentos inferior a minDF. Si minDF=1minDF=1, no se eliminará ninguna característica.\n",
    "* vocabSize: mantiene los términos de las frecuencias superiores de vocabSize.\n",
    "\n",
    "En el ejemplo siguiente, el `minTF=1.0,minDF=1.0minTF=1.0,minDF=1.0` y `vocabSize=20vocabSize=20`, que es mayor que el número total de términos. Por lo tanto, todas las características (términos) se mantendrán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "countvectorizer = CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=20, \n",
    "                                  inputCol='terms', outputCol='features(vocabSize), [index], [term frequency]')\n",
    "stages = [countvectorizer]\n",
    "pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+----------------------------------------------+\n",
      "|terms                                      |features(vocabSize), [index], [term frequency]|\n",
      "+-------------------------------------------+----------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(10,[0,1,3],[3.0,2.0,1.0])                    |\n",
      "|[I, love, spark, very, very, much]         |(10,[0,2,4,5,6],[1.0,2.0,1.0,1.0,1.0])        |\n",
      "|[everyone, should, use, spark]             |(10,[0,7,8,9],[1.0,1.0,1.0,1.0])              |\n",
      "+-------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usemos el StringIndexer() para indexar el corpus y ver si los resultados son consistentes con el método CountVectorizer().\n",
    "\n",
    "### `FlatMap` para que cada fila tenga un solo término."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   terms|\n",
      "+--------+\n",
      "|   spark|\n",
      "|   spark|\n",
      "|   spark|\n",
      "|      is|\n",
      "| awesome|\n",
      "| awesome|\n",
      "|       I|\n",
      "|    love|\n",
      "|   spark|\n",
      "|    very|\n",
      "|    very|\n",
      "|    much|\n",
      "|everyone|\n",
      "|  should|\n",
      "|     use|\n",
      "|   spark|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "df_vocab = df.select('terms').rdd.\\\n",
    "            flatMap(lambda x: x[0]).\\\n",
    "            toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular frecuencias de término en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|frequency|      term|\n",
      "+---------+----------+\n",
      "|        5|   [spark]|\n",
      "|        2|    [very]|\n",
      "|        2| [awesome]|\n",
      "|        1|      [is]|\n",
      "|        1|       [I]|\n",
      "|        1|    [love]|\n",
      "|        1|[everyone]|\n",
      "|        1|     [use]|\n",
      "|        1|    [much]|\n",
      "|        1|  [should]|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_freq = df_vocab.rdd.countByValue()\n",
    "pdf = pd.DataFrame({\n",
    "        'term': list(vocab_freq.keys()),\n",
    "        'frequency': list(vocab_freq.values())\n",
    "    })\n",
    "pdf\n",
    "tf = spark.createDataFrame(pdf).orderBy('frequency', ascending=False)\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicar `StringIndexer()` a df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringindexer = StringIndexer(inputCol='terms', outputCol='StringIndexer(index)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   terms|StringIndexer(index)|\n",
      "+--------+--------------------+\n",
      "|   spark|                 0.0|\n",
      "| awesome|                 1.0|\n",
      "|    very|                 2.0|\n",
      "|      is|                 3.0|\n",
      "|everyone|                 4.0|\n",
      "|       I|                 5.0|\n",
      "|    love|                 6.0|\n",
      "|  should|                 7.0|\n",
      "|    much|                 8.0|\n",
      "|     use|                 9.0|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringindexer.fit(df_vocab).transform(df_vocab).\\\n",
    "    distinct().\\\n",
    "    orderBy('StringIndexer(index)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de la indexación es consistente para los tres primeros trimestres. El resto de términos tienen la misma frecuencia que es 1. Estos términos no pueden ser ordenados por frecuencia. Esta podría ser la razón por la que sus índices no coinciden con los resultados del método CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
