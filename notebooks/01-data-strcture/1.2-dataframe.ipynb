{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objeto DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear SparkContext y SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear puntos de entrada para spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear objeto DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear DataFrame leyendo un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|_c0              |mpg |cyl|disp |hp |drat|wt   |qsec |vs |am |gear|carb|\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|Mazda RX4        |21.0|6  |160.0|110|3.9 |2.62 |16.46|0  |1  |4   |4   |\n",
      "|Mazda RX4 Wag    |21.0|6  |160.0|110|3.9 |2.875|17.02|0  |1  |4   |4   |\n",
      "|Datsun 710       |22.8|4  |108.0|93 |3.85|2.32 |18.61|1  |1  |4   |1   |\n",
      "|Hornet 4 Drive   |21.4|6  |258.0|110|3.08|3.215|19.44|1  |0  |3   |1   |\n",
      "|Hornet Sportabout|18.7|8  |360.0|175|3.15|3.44 |17.02|0  |0  |3   |2   |\n",
      "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mtcars = spark.read.csv(path='../../data/mtcars.csv',\n",
    "                        sep=',',\n",
    "                        encoding='UTF-8',\n",
    "                        comment=None,\n",
    "                        header=True, \n",
    "                        inferSchema=True)\n",
    "mtcars.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear DataFrame con la funcion `createDataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desde un RDD\n",
    "\n",
    "Los elementos en RDD tienen que ser un objeto Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=[1, 2, 3], y=['a', 'b', 'c']), Row(x=[4, 5, 6], y=['e', 'f', 'g'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([\n",
    "    Row(x=[1,2,3], y=['a','b','c']),\n",
    "    Row(x=[4,5,6], y=['e','f','g'])\n",
    "])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|        x|        y|\n",
      "+---------+---------+\n",
      "|[1, 2, 3]|[a, b, c]|\n",
      "|[4, 5, 6]|[e, f, g]|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con la libreria pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[a, b, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>[e, f, g]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y\n",
       "0  [1, 2, 3]  [a, b, c]\n",
       "1  [4, 5, 6]  [e, f, g]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "    'x': [[1,2,3], [4,5,6]],\n",
    "    'y': [['a','b','c'], ['e','f','g']]\n",
    "})\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|        x|        y|\n",
      "+---------+---------+\n",
      "|[1, 2, 3]|[a, b, c]|\n",
      "|[4, 5, 6]|[e, f, g]|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De una lista\n",
    "\n",
    "Cada elemento de la lista se convierte en una Fila en el DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     b|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_list = [['a', 1], ['b', 2]]\n",
    "df = spark.createDataFrame(my_list, ['letter', 'number'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('letter', 'string'), ('number', 'bigint')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|my_column| _2|\n",
      "+---------+---+\n",
      "|        a|  1|\n",
      "|        b|  2|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_list = [['a', 1], ['b', 2]]\n",
    "df = spark.createDataFrame(my_list, ['my_column'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my_column', 'string'), ('_2', 'bigint')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código genera un DataFrame que consiste en dos columnas, cada columna es una columna vectorial.\n",
    "\n",
    "¿Por qué se generan columnas vectoriales en este caso?\n",
    "En este caso, la lista **my_list** tiene sólo un elemento, una tupla. Por lo tanto, el DataFrame sólo tiene una fila. Esta tupla tiene dos elementos. Por lo tanto, genera un DataFrame de dos columnas. Cada elemento de la tupla es una lista, por lo que las columnas resultantes son columnas vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|     x|     y|\n",
      "+------+------+\n",
      "|[a, 1]|[b, 2]|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_list = [(['a', 1], ['b', 2])]\n",
    "df = spark.createDataFrame(my_list, ['x', 'y'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Instancia Column\n",
    "\n",
    "Las instancias de columna se pueden crear de dos maneras:\n",
    "\n",
    "1. Seleccione directamente una columna de entre un *DataFrame*: `df.colName`\n",
    "2. crear a partir de una expresión de columna: `df.colName + 1`\n",
    "\n",
    "Técnicamente, sólo hay una forma de crear una instancia de columna. Las expresiones de columna comienzan desde una instancia de columna.\n",
    "**Recuerde cómo crear instancias de columna, ya que éste es normalmente el punto de partida si queremos operar con columnas DataFrame.**\n",
    "\n",
    "Las clases de columna vienen con algunos métodos que pueden operar en una instancia de columna. ***Sin embargo, casi todas las funciones de `pyspark.sql.functions` tomar una o más instancias de columna como argumentos***. Estas funciones son importantes para las herramientas de manipulación de datos.\n",
    "\n",
    "## DataFrame metodos column\n",
    "\n",
    "### Methods that take column names as arguments:\n",
    "\n",
    "* `corr(col1, col2)`: nombres de dos columnas.\n",
    "* `cov(col1, col2)`: nombres de dos columnas.\n",
    "* `crosstab(col1, col2)`: nombres de dos columnas.\n",
    "* `describe(*cols)`: ***`*cols` se refiere sólo a nombres de columna (strings).***\n",
    "\n",
    "### Métodos que toman nombres de columnas o expresiones de columnas o **both** como argumentos:\n",
    "\n",
    "* `cube(*cols)`: nombres de columna (string) o expresiones de columna o **both**.\n",
    "* `drop(*cols)`: ***una lista de nombres de columnas O una expresión de una sola columna.***\n",
    "* `groupBy(*cols)`: nombre de columna (string) o expresión de columna o **both**.\n",
    "* `rollup(*cols)`: nombre de columna (string) o expresión de columna o **both**.\n",
    "* `select(*cols)`: nombre de columna (string) o expresión de columna o **both**.\n",
    "* `sort(*cols, **kwargs)`: column name (string) or column expression or **both**.\n",
    "* `sortWithinPartitions(*cols, **kwargs)`: nombre de columna (string) o expresión de columna o **both**.\n",
    "* `orderBy(*cols, **kwargs)`: nombre de columna (string) o expresión de columna o **both**.\n",
    "* `sampleBy(col, fractions, sed=None)`: nombre de columna.\n",
    "* `toDF(*cols)`: **una lista de nombres de columnas (string).**\n",
    "* `withColumn(colName, col)`: `colName` se refiere al nombre de la columna; `col` se refiere a una expresión de columna.\n",
    "* `withColumnRenamed(existing, new)`: toma los nombres de las columnas como argumentos.\n",
    "* `filter(condition)`: ***condicion** se refiere a una expresión de columna que devuelve `types.BooleanType` de valores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
